{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1fbb4ca",
   "metadata": {},
   "source": [
    "# Creating feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "443b0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import csv\n",
    "import re\n",
    "from shapely.geometry import box, Point\n",
    "import glob \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a90046",
   "metadata": {},
   "source": [
    "## Preparing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf276f",
   "metadata": {},
   "source": [
    "### Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff2829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:2263\n",
      "Cell size in feet: 6561.67\n",
      "Grid has 625 raw cells before clipping\n",
      "Grid has 285 cells after clipping\n",
      "                                            geometry  region_id\n",
      "0  POLYGON ((919736.776 126690.037, 926298.442 12...          0\n",
      "1  POLYGON ((926298.442 126690.037, 931219.36 126...          1\n",
      "2  POLYGON ((919736.776 126690.037, 919736.776 12...          2\n",
      "3  POLYGON ((919736.776 133251.703, 919736.776 12...          3\n",
      "4  POLYGON ((932860.109 133251.703, 939421.776 13...          4\n"
     ]
    }
   ],
   "source": [
    "# Read borough shapefile\n",
    "nybb = gpd.read_file(\"nyc_shapefile/nybb.shp\")\n",
    "\n",
    "print(nybb.crs)\n",
    "# this should be EPSG:2263  (NAD 1983 StatePlane New York Long Island, feet)\n",
    "\n",
    "# Dissolve boroughs into a single NYC polygon\n",
    "nyc = nybb.dissolve()          # one row, geometry is union of all boroughs\n",
    "nyc = nyc.reset_index(drop=True)\n",
    "\n",
    "# Get bounding box of NYC in feet (since CRS is in feet)\n",
    "minx, miny, maxx, maxy = nyc.total_bounds\n",
    "\n",
    "# Cell size: 2 km in feet (using US survey foot)\n",
    "feet_per_meter = 1 / 0.3048006096012192\n",
    "cell_size_ft = 2000 * feet_per_meter   # about 6561.67 feet\n",
    "\n",
    "print(f\"Cell size in feet: {cell_size_ft:.2f}\")\n",
    "\n",
    "# Build grid of square polygons over the bounding box\n",
    "xs = np.arange(minx, maxx + cell_size_ft, cell_size_ft)\n",
    "ys = np.arange(miny, maxy + cell_size_ft, cell_size_ft)\n",
    "\n",
    "cells = []\n",
    "for x in xs:\n",
    "    for y in ys:\n",
    "        cell = box(x, y, x + cell_size_ft, y + cell_size_ft)\n",
    "        cells.append(cell)\n",
    "\n",
    "grid = gpd.GeoDataFrame({\"geometry\": cells}, crs=nyc.crs)\n",
    "\n",
    "print(f\"Grid has {len(grid)} raw cells before clipping\")\n",
    "\n",
    "# Clip grid to NYC outline\n",
    "grid_clipped = gpd.clip(grid, nyc)\n",
    "\n",
    "# Remove empty geometries if any\n",
    "grid_clipped = grid_clipped[~grid_clipped.is_empty].reset_index(drop=True)\n",
    "\n",
    "# Add region_id\n",
    "grid_clipped[\"region_id\"] = grid_clipped.index\n",
    "\n",
    "print(f\"Grid has {len(grid_clipped)} cells after clipping\")\n",
    "\n",
    "# Inspect\n",
    "print(grid_clipped.head())\n",
    "\n",
    "# Save shapefile\n",
    "grid_clipped.to_file(\"data/nyc_grid_2km.shp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ae3de",
   "metadata": {},
   "source": [
    "### NYPD complaint data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ab395f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CMPLNT_FR_DT</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lat_Lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07/01/2012</td>\n",
       "      <td>07/01/2012</td>\n",
       "      <td>(40.8441566000203, -73.9006054489734)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07/01/2012</td>\n",
       "      <td>07/01/2012</td>\n",
       "      <td>(40.6744956865259, -73.9305713255961)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07/01/2012</td>\n",
       "      <td>07/01/2012</td>\n",
       "      <td>(40.6984738177025, -73.917768981221)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07/01/2012</td>\n",
       "      <td>07/01/2012</td>\n",
       "      <td>(40.7565675846374, -73.8759315341335)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07/01/2012</td>\n",
       "      <td>07/01/2012</td>\n",
       "      <td>(40.7072398161698, -73.7927267255908)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CMPLNT_FR_DT        Date                                Lat_Lon\n",
       "0   07/01/2012  07/01/2012  (40.8441566000203, -73.9006054489734)\n",
       "1   07/01/2012  07/01/2012  (40.6744956865259, -73.9305713255961)\n",
       "2   07/01/2012  07/01/2012   (40.6984738177025, -73.917768981221)\n",
       "3   07/01/2012  07/01/2012  (40.7565675846374, -73.8759315341335)\n",
       "4   07/01/2012  07/01/2012  (40.7072398161698, -73.7927267255908)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaint = pd.read_csv('old_data/NYPD_Complaint_Data.csv')\n",
    "\n",
    "# turn empty strings into NaN\n",
    "complaint = complaint.replace(\"\", np.nan)\n",
    "\n",
    "# if xcoord and ycoord are strings that sometimes contain spaces:\n",
    "complaint[\"Lat_Lon\"] = complaint[\"Lat_Lon\"].astype(str).str.strip()\n",
    "complaint[\"Date\"] = complaint[\"CMPLNT_FR_DT\"].astype(str).str.strip()\n",
    "complaint = complaint.replace({\"Lat_Lon\": {\"\": np.nan},\n",
    "                         \"Date\": {\"\": np.nan}})\n",
    "\n",
    "# drop true missing values\n",
    "complaint = complaint.dropna(subset=[\"Date\", \"Lat_Lon\"])\n",
    "\n",
    "# filter columns\n",
    "complaint = complaint[['CMPLNT_FR_DT', 'Date', 'Lat_Lon']].copy()\n",
    "\n",
    "# save\n",
    "complaint.to_csv(\"data/Complaint.csv\", index=False)\n",
    "\n",
    "complaint.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f712f1e",
   "metadata": {},
   "source": [
    "### Search and frisk dataset (SAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457dfc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/4071828253.py:1: DtypeWarning: Columns (10,73,103,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sas12 = pd.read_csv(\"old_data/SAS_2012.csv\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/4071828253.py:2: DtypeWarning: Columns (10,73,83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sas13 = pd.read_csv(\"old_data/SAS_2013.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>xcoord</th>\n",
       "      <th>ycoord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>337410</th>\n",
       "      <td>2012-07-02</td>\n",
       "      <td>1019585</td>\n",
       "      <td>184371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337411</th>\n",
       "      <td>2012-07-03</td>\n",
       "      <td>987078</td>\n",
       "      <td>215157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337412</th>\n",
       "      <td>2012-07-05</td>\n",
       "      <td>1002416</td>\n",
       "      <td>231297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337414</th>\n",
       "      <td>2012-07-06</td>\n",
       "      <td>988511</td>\n",
       "      <td>164316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337415</th>\n",
       "      <td>2012-07-08</td>\n",
       "      <td>995824</td>\n",
       "      <td>230943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date   xcoord  ycoord\n",
       "337410  2012-07-02  1019585  184371\n",
       "337411  2012-07-03   987078  215157\n",
       "337412  2012-07-05  1002416  231297\n",
       "337414  2012-07-06   988511  164316\n",
       "337415  2012-07-08   995824  230943"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas12 = pd.read_csv(\"old_data/SAS_2012.csv\")\n",
    "sas13 = pd.read_csv(\"old_data/SAS_2013.csv\")\n",
    "\n",
    "def add_date_column(df):\n",
    "    # to string\n",
    "    s = df[\"datestop\"].astype(str)\n",
    "    # pad to 8 characters, e.g. \"1012012\" -> \"01012012\"\n",
    "    s = s.str.zfill(8)\n",
    "    # parse as MMDDYYYY\n",
    "    df[\"date\"] = pd.to_datetime(s, format=\"%m%d%Y\")\n",
    "    return df\n",
    "\n",
    "sas12 = add_date_column(sas12)\n",
    "sas13 = add_date_column(sas13)\n",
    "\n",
    "# combine both years\n",
    "sas = pd.concat([sas12, sas13], ignore_index=True)\n",
    "\n",
    "# filter to 2012-07-01 through 2013-06-30\n",
    "start = pd.Timestamp(\"2012-07-01\")\n",
    "end   = pd.Timestamp(\"2013-06-30\")\n",
    "\n",
    "sas_df = sas[(sas[\"date\"] >= start) & (sas[\"date\"] <= end)]\n",
    "\n",
    "# turn empty strings into NaN\n",
    "sas_df = sas_df.replace(\"\", np.nan)\n",
    "\n",
    "# if xcoord and ycoord are strings that sometimes contain spaces:\n",
    "sas_df[\"xcoord\"] = sas_df[\"xcoord\"].astype(str).str.strip()\n",
    "sas_df[\"ycoord\"] = sas_df[\"ycoord\"].astype(str).str.strip()\n",
    "sas_df[\"Date\"] = sas_df[\"date\"].astype(str).str.strip()\n",
    "sas_df = sas_df.replace({\"xcoord\": {\"\": np.nan},\n",
    "                         \"ycoord\": {\"\": np.nan},\n",
    "                         \"date\": {\"\": np.nan}})\n",
    "\n",
    "# drop true missing values\n",
    "sas_df = sas_df.dropna(subset=[\"Date\", \"xcoord\", \"ycoord\"])\n",
    "\n",
    "# filter columns\n",
    "sas_df = sas_df[['Date', 'xcoord', 'ycoord']].copy()\n",
    "\n",
    "# save\n",
    "sas_df.to_csv(\"data/SAS.csv\", index=False)\n",
    "\n",
    "sas_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912087f",
   "metadata": {},
   "source": [
    "### 311 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790eb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique Key</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lat_Lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25847928</td>\n",
       "      <td>2013-06-30 12:09:56</td>\n",
       "      <td>(40.704084501215796, -73.90740226066829)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25849947</td>\n",
       "      <td>2013-06-30 12:09:55</td>\n",
       "      <td>(40.69569076684871, -73.73678889778218)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25850028</td>\n",
       "      <td>2013-06-30 12:09:34</td>\n",
       "      <td>(40.72260022794115, -73.86761631518405)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25847685</td>\n",
       "      <td>2013-06-30 12:08:43</td>\n",
       "      <td>(40.639048553176586, -74.00332571465987)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25847702</td>\n",
       "      <td>2013-06-30 12:08:19</td>\n",
       "      <td>(40.77814711286228, -73.9196478173007)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique Key                Date                                   Lat_Lon\n",
       "0    25847928 2013-06-30 12:09:56  (40.704084501215796, -73.90740226066829)\n",
       "1    25849947 2013-06-30 12:09:55   (40.69569076684871, -73.73678889778218)\n",
       "3    25850028 2013-06-30 12:09:34   (40.72260022794115, -73.86761631518405)\n",
       "4    25847685 2013-06-30 12:08:43  (40.639048553176586, -74.00332571465987)\n",
       "5    25847702 2013-06-30 12:08:19    (40.77814711286228, -73.9196478173007)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_311 = pd.read_csv(\"old_data/311_Service_Requests.csv\")\n",
    "\n",
    "# Parse date directly, coercing bad ones to NaN\n",
    "df_311[\"Date\"] = pd.to_datetime(df_311[\"Created Date\"], errors=\"coerce\")\n",
    "\n",
    "# Copy Location to Lat_Lon without astype(str) first\n",
    "df_311[\"Lat_Lon\"] = df_311[\"Location\"]\n",
    "\n",
    "# Treat blanks / whitespace / obvious junk as missing\n",
    "df_311[\"Lat_Lon\"] = df_311[\"Lat_Lon\"].replace(\n",
    "    [ \"\", \" \", \"  \", \"nan\", \"NaN\" ], np.nan\n",
    ")\n",
    "# also strip whitespace then turn pure-whitespace into NaN\n",
    "df_311[\"Lat_Lon\"] = df_311[\"Lat_Lon\"].astype(str).str.strip()\n",
    "df_311[\"Lat_Lon\"] = df_311[\"Lat_Lon\"].replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan})\n",
    "\n",
    "# Drop rows missing either Date or Lat_Lon\n",
    "df_311 = df_311.dropna(subset=[\"Date\", \"Lat_Lon\"])\n",
    "\n",
    "# Keep only the needed columns\n",
    "df_311 = df_311[[\"Unique Key\", \"Date\", \"Lat_Lon\"]].copy()\n",
    "\n",
    "df_311.to_csv(\"data/311.csv\", index=False)\n",
    "\n",
    "df_311.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1af76c",
   "metadata": {},
   "source": [
    "### Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f70ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TMAX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-07-01</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-02</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-03</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-04</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-05</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PRCP  SNOW  TMIN  TMAX\n",
       "DATE                              \n",
       "2012-07-01  0.00   0.0  74.0  94.0\n",
       "2012-07-02  0.00   0.0  72.0  88.0\n",
       "2012-07-03  0.00   0.0  71.0  89.0\n",
       "2012-07-04  0.06   0.0  70.0  92.0\n",
       "2012-07-05  0.00   0.0  77.0  95.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and normalize column names\n",
    "weather = pd.read_csv(\"old_data/NYC_Central_Park_weather_1869-2022.csv\")\n",
    "weather.columns = weather.columns.str.strip().str.upper()\n",
    "\n",
    "# Parse dates and sort\n",
    "weather[\"DATE\"] = pd.to_datetime(weather[\"DATE\"])\n",
    "weather = weather.sort_values(\"DATE\")\n",
    "\n",
    "# Keep only time window\n",
    "start = pd.Timestamp(\"2012-07-01\")\n",
    "end   = pd.Timestamp(\"2013-06-30\")\n",
    "\n",
    "weather = weather[(weather[\"DATE\"] >= start) & (weather[\"DATE\"] <= end)].copy()\n",
    "\n",
    "# Make sure to have a row for every day\n",
    "all_days = pd.date_range(start, end, freq=\"D\")\n",
    "weather = weather.set_index(\"DATE\").reindex(all_days)\n",
    "weather.index.name = \"DATE\"\n",
    "\n",
    "# Handle missing values\n",
    "# treat missing precipitation and snow as 0\n",
    "for col in [\"PRCP\", \"SNOW\", \"SNWD\"]:\n",
    "    if col in weather.columns:\n",
    "        weather[col] = weather[col].fillna(0.0)\n",
    "\n",
    "# interpolate missing temps\n",
    "for col in [\"TMIN\", \"TMAX\"]:\n",
    "    if col in weather.columns:\n",
    "        weather[col] = weather[col].interpolate(limit_direction=\"both\")\n",
    "\n",
    "\n",
    "weather_clean = weather[[\"PRCP\", \"SNOW\", \"TMIN\", \"TMAX\"]].copy()\n",
    "\n",
    "weather_clean.to_csv(\"data/weather.csv\")\n",
    "\n",
    "weather_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5663c7",
   "metadata": {},
   "source": [
    "### POIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b2cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>venue_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3fd66200f964a52000e71ee3</td>\n",
       "      <td>40.733596</td>\n",
       "      <td>-74.003139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3fd66200f964a52000e81ee3</td>\n",
       "      <td>40.758102</td>\n",
       "      <td>-73.975734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3fd66200f964a52000ea1ee3</td>\n",
       "      <td>40.732456</td>\n",
       "      <td>-74.003755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3fd66200f964a52000ec1ee3</td>\n",
       "      <td>42.345907</td>\n",
       "      <td>-71.087001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3fd66200f964a52000ee1ee3</td>\n",
       "      <td>39.933178</td>\n",
       "      <td>-75.159262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   venue_id        lat        lon\n",
       "0  3fd66200f964a52000e71ee3  40.733596 -74.003139\n",
       "1  3fd66200f964a52000e81ee3  40.758102 -73.975734\n",
       "2  3fd66200f964a52000ea1ee3  40.732456 -74.003755\n",
       "3  3fd66200f964a52000ec1ee3  42.345907 -71.087001\n",
       "4  3fd66200f964a52000ee1ee3  39.933178 -75.159262"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_in  = \"old_data/POIs.txt\"\n",
    "poi_out = \"data/POIs_clean.csv\"\n",
    "\n",
    "# read whole POI file, large but manageable\n",
    "pois = pd.read_csv(\n",
    "    poi_in,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"venue_id\", \"lat\", \"lon\", \"category\", \"country\"],\n",
    "    dtype={\"venue_id\": str, \"category\": str, \"country\": str},\n",
    ")\n",
    "\n",
    "# strip whitespace\n",
    "for col in [\"venue_id\", \"category\", \"country\"]:\n",
    "    pois[col] = pois[col].astype(str).str.strip()\n",
    "\n",
    "# coerce lat/lon to numeric\n",
    "pois[\"lat\"] = pd.to_numeric(pois[\"lat\"], errors=\"coerce\")\n",
    "pois[\"lon\"] = pd.to_numeric(pois[\"lon\"], errors=\"coerce\")\n",
    "\n",
    "# check on coordinates\n",
    "pois = pois[\n",
    "    (pois[\"lat\"].between(-90, 90)) &\n",
    "    (pois[\"lon\"].between(-180, 180))\n",
    "]\n",
    "\n",
    "# drop rows missing venue_id or coords\n",
    "pois = pois.dropna(subset=[\"venue_id\", \"lat\", \"lon\"])\n",
    "\n",
    "# keep minimal columns\n",
    "pois_clean = pois[[\"venue_id\", \"lat\", \"lon\"]].copy()\n",
    "\n",
    "pois_clean.to_csv(poi_out, index=False)\n",
    "\n",
    "pois_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6831a83",
   "metadata": {},
   "source": [
    "### Check-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743918d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
      "/var/folders/6f/lxkzpyyn0fvcj0qtkc53g_2w0000gn/T/ipykernel_34226/1974790132.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>venue_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4f5e3a72e4b053fd6a4313f6</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>55.696132</td>\n",
       "      <td>37.557842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4b4b87b5f964a5204a9f26e3</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>41.029717</td>\n",
       "      <td>28.974420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4a85b1b3f964a520eefe1fe3</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>40.748939</td>\n",
       "      <td>-73.992280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4b4606f2f964a520751426e3</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>30.270753</td>\n",
       "      <td>-97.752936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4c2b4e8a9a559c74832f0de2</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>59.941041</td>\n",
       "      <td>30.308104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   venue_id        date        lat        lon\n",
       "0  4f5e3a72e4b053fd6a4313f6  2012-04-03  55.696132  37.557842\n",
       "1  4b4b87b5f964a5204a9f26e3  2012-04-03  41.029717  28.974420\n",
       "2  4a85b1b3f964a520eefe1fe3  2012-04-03  40.748939 -73.992280\n",
       "3  4b4606f2f964a520751426e3  2012-04-03  30.270753 -97.752936\n",
       "4  4c2b4e8a9a559c74832f0de2  2012-04-03  59.941041  30.308104"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_in = \"old_data/Checkins.txt\"\n",
    "checkins_merged_out = \"data/checkins_merged.csv\"\n",
    "\n",
    "chunk_size = 1000000\n",
    "header_written = False\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    check_in,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"venue_id\", \"utc_time\", \"offset_min\"],,\n",
    "    chunksize=chunk_size,\n",
    "):\n",
    "    # strip whitespace\n",
    "    for col in [\"venue_id\", \"utc_time\"]:\n",
    "        chunk[col] = chunk[col].astype(str).str.strip()\n",
    "\n",
    "    # clean offset and drop bad rows\n",
    "    chunk[\"offset_min\"] = pd.to_numeric(chunk[\"offset_min\"], errors=\"coerce\")\n",
    "    chunk = chunk.dropna(subset=[\"venue_id\", \"utc_time\", \"offset_min\"])\n",
    "\n",
    "    # compute utc_dt/local_time/date on the chunk\n",
    "    chunk[\"utc_dt\"] = pd.to_datetime(chunk[\"utc_time\"], errors=\"coerce\")\n",
    "    chunk = chunk.dropna(subset=[\"utc_dt\"])\n",
    "    chunk[\"local_time\"] = chunk[\"utc_dt\"] + pd.to_timedelta(chunk[\"offset_min\"], unit=\"m\")\n",
    "    chunk[\"date\"] = chunk[\"local_time\"].dt.date\n",
    "\n",
    "    # merge with POIs on the chunk\n",
    "    chunk = chunk.merge(pois_clean[[\"venue_id\", \"lat\", \"lon\"]],\n",
    "                        on=\"venue_id\", how=\"inner\")\n",
    "\n",
    "    # keep minimal cols\n",
    "    chunk = chunk[[\"venue_id\", \"date\", \"lat\", \"lon\"]]\n",
    "\n",
    "    # append result to output CSV\n",
    "    mode = \"w\" if not header_written else \"a\"\n",
    "    chunk.to_csv(checkins_merged_out, mode=mode, index=False, header=not header_written)\n",
    "    header_written = True\n",
    "\n",
    "checkins_merged = pd.read_csv(checkins_merged_out)\n",
    "checkins_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb8514",
   "metadata": {},
   "source": [
    "### Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7efdcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "input_paths_2012 = [f\"old_data/trip_data_{m}.csv\" for m in range(7, 13)]\n",
    "\n",
    "out_2012 = \"data/taxi_clean_2012_H2.csv\"\n",
    "\n",
    "with open(out_2012, \"w\", encoding=\"utf8\", newline=\"\") as out:\n",
    "    writer = csv.writer(out)\n",
    "    writer.writerow([\"pickup_datetime\", \"lon\", \"lat\"])\n",
    "\n",
    "    total_written = 0\n",
    "\n",
    "    for input_path in input_paths_2012:\n",
    "        with open(input_path, \"r\", encoding=\"utf8\", errors=\"replace\", newline=\"\") as inp:\n",
    "            reader = csv.reader(inp)\n",
    "            header = next(reader, None)\n",
    "            if not header:\n",
    "                continue\n",
    "\n",
    "            col = {name.strip().lower(): i for i, name in enumerate(header)}\n",
    "\n",
    "            dt_i = col.get(\"pickup_datetime\")\n",
    "            lon_i = col.get(\"pickup_longitude\")\n",
    "            lat_i = col.get(\"pickup_latitude\")\n",
    "\n",
    "            if dt_i is None or lon_i is None or lat_i is None:\n",
    "                raise ValueError(f\"Missing expected columns in {input_path}. Header starts: {header[:20]}\")\n",
    "\n",
    "            for row in reader:\n",
    "                if len(row) <= max(dt_i, lon_i, lat_i):\n",
    "                    continue\n",
    "\n",
    "                dt_raw = row[dt_i].strip()\n",
    "                lon = row[lon_i].strip()\n",
    "                lat = row[lat_i].strip()\n",
    "\n",
    "                if not dt_raw or not lon or not lat:\n",
    "                    continue\n",
    "\n",
    "                # drop obvious bad coords\n",
    "                if lon in (\"0\", \"0.0\") or lat in (\"0\", \"0.0\"):\n",
    "                    continue\n",
    "\n",
    "                writer.writerow([dt_raw, lon, lat])\n",
    "                total_written += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09d4c6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: old_data/trip_data_1.csv\n",
      "Processing: old_data/trip_data_2.csv\n",
      "Processing: old_data/trip_data_3.csv\n",
      "Processing: old_data/trip_data_4.csv\n",
      "Processing: old_data/trip_data_5.csv\n",
      "Processing: old_data/trip_data_6.csv\n",
      "Wrote: data/taxi_clean_2013_H1.csv rows: 86260732\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "input_paths_2013 = [f\"old_data/trip_data_{m}.csv\" for m in range(1, 7)]\n",
    "out_2013 = \"data/taxi_clean_2013_H1.csv\"\n",
    "\n",
    "with open(out_2013, \"w\", encoding=\"utf8\", newline=\"\") as out:\n",
    "    writer = csv.writer(out)\n",
    "    writer.writerow([\"pickup_datetime\", \"lon\", \"lat\"])\n",
    "\n",
    "    total_written = 0\n",
    "\n",
    "    for input_path in input_paths_2013:\n",
    "        print(\"Processing:\", input_path)\n",
    "\n",
    "        with open(input_path, \"r\", encoding=\"utf8\", errors=\"replace\", newline=\"\") as inp:\n",
    "            # strip NUL bytes while streaming\n",
    "            reader = csv.reader((line.replace(\"\\x00\", \"\") for line in inp))\n",
    "\n",
    "            header = next(reader, None)\n",
    "            if not header:\n",
    "                continue\n",
    "\n",
    "            col = {name.strip().lower(): i for i, name in enumerate(header)}\n",
    "\n",
    "            dt_i = col.get(\"pickup_datetime\")\n",
    "            lon_i = col.get(\"pickup_longitude\")\n",
    "            lat_i = col.get(\"pickup_latitude\")\n",
    "\n",
    "            if dt_i is None or lon_i is None or lat_i is None:\n",
    "                raise ValueError(f\"Missing expected columns in {input_path}. Header starts: {header[:20]}\")\n",
    "\n",
    "            for row in reader:\n",
    "                if len(row) <= max(dt_i, lon_i, lat_i):\n",
    "                    continue\n",
    "\n",
    "                dt_raw = row[dt_i].strip()\n",
    "                lon = row[lon_i].strip()\n",
    "                lat = row[lat_i].strip()\n",
    "\n",
    "                if not dt_raw or not lon or not lat:\n",
    "                    continue\n",
    "\n",
    "                if lon in (\"0\", \"0.0\") or lat in (\"0\", \"0.0\"):\n",
    "                    continue\n",
    "\n",
    "                writer.writerow([dt_raw, lon, lat])\n",
    "                total_written += 1\n",
    "\n",
    "print(\"Wrote:\", out_2013, \"rows:\", total_written)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10db5847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final rows: 170629698\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-07-01 00:00:00</td>\n",
       "      <td>-73.872940</td>\n",
       "      <td>40.774117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-07-01 00:00:00</td>\n",
       "      <td>-73.983932</td>\n",
       "      <td>40.725529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-07-01 00:00:00</td>\n",
       "      <td>-73.994675</td>\n",
       "      <td>40.726028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-07-01 00:00:00</td>\n",
       "      <td>-73.979927</td>\n",
       "      <td>40.752216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-07-01 00:00:00</td>\n",
       "      <td>-73.979797</td>\n",
       "      <td>40.749989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pickup_datetime        lon        lat\n",
       "0  2012-07-01 00:00:00 -73.872940  40.774117\n",
       "1  2012-07-01 00:00:00 -73.983932  40.725529\n",
       "2  2012-07-01 00:00:00 -73.994675  40.726028\n",
       "3  2012-07-01 00:00:00 -73.979927  40.752216\n",
       "4  2012-07-01 00:00:00 -73.979797  40.749989"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_2012 = \"data/taxi_clean_2012_H2.csv\"\n",
    "out_2013 = \"data/taxi_clean_2013_H1.csv\"\n",
    "final_out = \"data/taxi_clean.csv\"\n",
    "\n",
    "with open(final_out, \"w\", encoding=\"utf8\", newline=\"\") as out:\n",
    "    writer = csv.writer(out)\n",
    "    writer.writerow([\"pickup_datetime\", \"lon\", \"lat\"])\n",
    "\n",
    "    for part in (out_2012, out_2013):\n",
    "        with open(part, \"r\", encoding=\"utf8\", errors=\"replace\", newline=\"\") as inp:\n",
    "            reader = csv.reader(inp)\n",
    "            next(reader, None)  # skip header\n",
    "            for row in reader:\n",
    "                if len(row) == 3:\n",
    "                    writer.writerow(row)\n",
    "\n",
    "taxi = pd.read_csv(final_out)\n",
    "print(\"Final rows:\", len(taxi))\n",
    "taxi.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1529a",
   "metadata": {},
   "source": [
    "## Integrating grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad682109",
   "metadata": {},
   "source": [
    "- what if events overlap regions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c2270",
   "metadata": {},
   "source": [
    "### Complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9251078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint = pd.read_csv('data/Complaint.csv')\n",
    "grid_clipped = gpd.read_file('data/nyc_grid_2km.shp')\n",
    "\n",
    "# Remove parentheses and split into two parts\n",
    "lat_lon_split = complaint[\"Lat_Lon\"].str.strip(\"()\").str.split(\",\", expand=True)\n",
    "\n",
    "complaint[\"lat\"] = lat_lon_split[0].astype(float)\n",
    "complaint[\"lon\"] = lat_lon_split[1].astype(float)\n",
    "\n",
    "gdf_complaint = gpd.GeoDataFrame(\n",
    "    complaint,\n",
    "    geometry=gpd.points_from_xy(complaint[\"lon\"], complaint[\"lat\"]),\n",
    "    crs=\"EPSG:4326\",       # WGS84\n",
    ")\n",
    "\n",
    "gdf_complaint = gdf_complaint.to_crs(grid_clipped.crs)  # use gdf_complaint\n",
    "\n",
    "joined_complaint = gpd.sjoin(\n",
    "    gdf_complaint,\n",
    "    grid_clipped[[\"region_id\", \"geometry\"]],\n",
    "    how=\"inner\",\n",
    "    predicate=\"within\",\n",
    ")\n",
    "\n",
    "joined_complaint[\"date\"] = pd.to_datetime(joined_complaint[\"Date\"]).dt.date \n",
    "\n",
    "complaint_daily = (\n",
    "    joined_complaint\n",
    "    .groupby([\"region_id\", \"date\"])\n",
    "    .size()\n",
    "    .rename(\"complaint_count\")\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f238ba6",
   "metadata": {},
   "source": [
    "### SAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_df = pd.read_csv('data/SAS.csv')\n",
    "grid_clipped = gpd.read_file('data/nyc_grid_2km.shp')\n",
    "\n",
    "# turn into GeoDataFrame, same CRS as grid\n",
    "gdf_sas = gpd.GeoDataFrame(\n",
    "    sas_df,\n",
    "    geometry=gpd.points_from_xy(sas_df[\"xcoord\"], sas_df[\"ycoord\"]),\n",
    "    crs=grid_clipped.crs\n",
    ")\n",
    "\n",
    "# spatial join: which grid cell is each stop in\n",
    "sas_join = gpd.sjoin(\n",
    "    gdf_sas,\n",
    "    grid_clipped[[\"region_id\", \"geometry\"]],\n",
    "    how=\"inner\",\n",
    "    predicate=\"within\",\n",
    ")\n",
    "\n",
    "sas_join[\"date\"] = pd.to_datetime(sas_join[\"Date\"]).dt.date \n",
    "\n",
    "# aggregate to region x day\n",
    "sas_daily = (\n",
    "    sas_join\n",
    "    .groupby([\"region_id\", \"date\"])\n",
    "    .size()\n",
    "    .rename(\"sas_count\")\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d2b4e",
   "metadata": {},
   "source": [
    "### 311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afb95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_311 = pd.read_csv('data/311.csv')\n",
    "grid_clipped = gpd.read_file('data/nyc_grid_2km.shp')\n",
    "\n",
    "# Remove parentheses and split into two parts\n",
    "lat_lon_split = df_311[\"Lat_Lon\"].str.strip(\"()\").str.split(\",\", expand=True)\n",
    "\n",
    "df_311[\"lat\"] = lat_lon_split[0].astype(float)\n",
    "df_311[\"lon\"] = lat_lon_split[1].astype(float)\n",
    "\n",
    "gdf_311 = gpd.GeoDataFrame(\n",
    "    df_311,\n",
    "    geometry=gpd.points_from_xy(df_311[\"lon\"], df_311[\"lat\"]),\n",
    "    crs=\"EPSG:4326\",       # WGS84\n",
    ")\n",
    "\n",
    "gdf_311 = gdf_311.to_crs(grid_clipped.crs) # project to State Plane like grid\n",
    "\n",
    "joined_311 = gpd.sjoin(\n",
    "    gdf_311,\n",
    "    grid_clipped[[\"region_id\", \"geometry\"]],\n",
    "    how=\"inner\",\n",
    "    predicate=\"within\",\n",
    ")\n",
    "\n",
    "joined_311[\"date\"] = pd.to_datetime(joined_311[\"Date\"]).dt.date\n",
    "\n",
    "daily_311 = (\n",
    "    joined_311\n",
    "    .groupby([\"region_id\", \"date\"])\n",
    "    .size()\n",
    "    .rename(\"311_count\")\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623bff00",
   "metadata": {},
   "source": [
    "### Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c32cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d022e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather will be divided over all regions for every day\n",
    "weather_clean = pd.read_csv('data/weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2e7c0",
   "metadata": {},
   "source": [
    "### POIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96df2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static so no date join\n",
    "\n",
    "pois_clean = pd.read_csv('data/POIs_clean.csv')\n",
    "grid_clipped = gpd.read_file('data/nyc_grid_2km.shp')\n",
    "\n",
    "gdf_pois = gpd.GeoDataFrame(\n",
    "    pois_clean,\n",
    "    geometry=gpd.points_from_xy(pois_clean[\"lon\"], pois_clean[\"lat\"]),\n",
    "    crs=\"EPSG:4326\",       # WGS84\n",
    ")\n",
    "\n",
    "gdf_pois = gdf_pois.to_crs(grid_clipped.crs) # project to State Plane like grid\n",
    "\n",
    "joined_pois = gpd.sjoin(\n",
    "    gdf_pois,\n",
    "    grid_clipped[[\"region_id\", \"geometry\"]],\n",
    "    how=\"inner\",\n",
    "    predicate=\"within\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0bfac",
   "metadata": {},
   "source": [
    "### Checkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins_merged = pd.read_csv('data/checkins_merged.csv')\n",
    "grid_clipped = gpd.read_file('data/nyc_grid_2km.shp')\n",
    "\n",
    "gdf_checkins = gpd.GeoDataFrame(\n",
    "    checkins_merged,\n",
    "    geometry=gpd.points_from_xy(checkins_merged[\"lon\"], checkins_merged[\"lat\"]),\n",
    "    crs=\"EPSG:4326\",       # WGS84\n",
    ")\n",
    "\n",
    "gdf_checkins = gdf_checkins.to_crs(grid_clipped.crs) # project to State Plane like grid\n",
    "\n",
    "joined_checkins = gpd.sjoin(\n",
    "    gdf_checkins,\n",
    "    grid_clipped[[\"region_id\", \"geometry\"]],\n",
    "    how=\"inner\",\n",
    "    predicate=\"within\",\n",
    ")\n",
    "\n",
    "joined_checkins[\"date\"] = pd.to_datetime(joined_checkins[\"date\"]).dt.date\n",
    "\n",
    "checkin_daily = (\n",
    "    joined_checkins\n",
    "    .groupby([\"region_id\", \"date\"])\n",
    "    .size()\n",
    "    .rename(\"checkin_count\")\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140926d",
   "metadata": {},
   "source": [
    "### Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6a42fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_id</th>\n",
       "      <th>date</th>\n",
       "      <th>taxi_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-07-22</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-08-09</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-08-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region_id        date  taxi_count\n",
       "0          0  2012-07-22         1.0\n",
       "1          0  2012-08-01         1.0\n",
       "2          0  2012-08-09         2.0\n",
       "3          0  2012-08-16         1.0\n",
       "4          0  2012-08-26         1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Read grid once, convert once to WGS84 so no reprojecting taxi points\n",
    "grid_clipped = gpd.read_file(\"data/nyc_grid_2km.shp\")[[\"region_id\", \"geometry\"]].to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Build spatial index once (speeds up sjoin)\n",
    "_ = grid_clipped.sindex\n",
    "\n",
    "chunksize = 1_000_000 \n",
    "\n",
    "acc = None  # will hold running counts as a Series with MultiIndex (region_id, date)\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    \"data/taxi_clean.csv\",\n",
    "    usecols=[\"pickup_datetime\", \"lon\", \"lat\"],\n",
    "    dtype={\"lon\": \"float32\", \"lat\": \"float32\"},\n",
    "    parse_dates=[\"pickup_datetime\"],\n",
    "    chunksize=chunksize,\n",
    "):\n",
    "    # drop bad/missing coords quickly\n",
    "    chunk = chunk.dropna(subset=[\"pickup_datetime\", \"lon\", \"lat\"])\n",
    "    chunk = chunk[(chunk[\"lon\"] != 0) & (chunk[\"lat\"] != 0)]\n",
    "\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        chunk[[\"pickup_datetime\"]].copy(),\n",
    "        geometry=gpd.points_from_xy(chunk[\"lon\"], chunk[\"lat\"]),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "\n",
    "    joined = gpd.sjoin(\n",
    "        gdf,\n",
    "        grid_clipped,\n",
    "        how=\"inner\",\n",
    "        predicate=\"within\",\n",
    "    )\n",
    "\n",
    "    # aggregate immediately to keep memory low\n",
    "    joined[\"date\"] = joined[\"pickup_datetime\"].dt.date\n",
    "    counts = joined.groupby([\"region_id\", \"date\"]).size()\n",
    "\n",
    "    acc = counts if acc is None else acc.add(counts, fill_value=0)\n",
    "\n",
    "# Final taxi_daily dataframe\n",
    "taxi_daily = (\n",
    "    acc.rename(\"taxi_count\")\n",
    "       .reset_index()\n",
    "       .sort_values([\"region_id\", \"date\"])\n",
    "       .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "taxi_daily.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90d3e1",
   "metadata": {},
   "source": [
    "## Build feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7623baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine regions with crime\n",
    "total_by_region = (\n",
    "    complaint_daily\n",
    "    .groupby(\"region_id\")[\"complaint_count\"]\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "active_regions = total_by_region[total_by_region > 0].index\n",
    "len(active_regions) # dropped 284 - 250 = 44 regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74225715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all feature tables for active regions\n",
    "grid_active        = grid_clipped[grid_clipped[\"region_id\"].isin(active_regions)].copy()\n",
    "complaint_daily    = complaint_daily[complaint_daily[\"region_id\"].isin(active_regions)].copy()\n",
    "sas_daily          = sas_daily[sas_daily[\"region_id\"].isin(active_regions)].copy()\n",
    "daily_311          = daily_311[daily_311[\"region_id\"].isin(active_regions)].copy()\n",
    "checkin_daily      = checkin_daily[checkin_daily[\"region_id\"].isin(active_regions)].copy() \n",
    "taxi_daily         = taxi_daily[taxi_daily[\"region_id\"].isin(active_regions)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20767376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full index using only active regions\n",
    "regions = sorted(active_regions)\n",
    "dates = pd.date_range(\"2012-07-01\", \"2013-06-30\", freq=\"D\")\n",
    "\n",
    "full_index = pd.MultiIndex.from_product(\n",
    "    [regions, dates],\n",
    "    names=[\"region_id\", \"date\"]\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(index=full_index).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "227de6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize 'date' to datetime\n",
    "X['date'] = pd.to_datetime(X['date'])\n",
    "sas_daily['date'] = pd.to_datetime(sas_daily['date'])\n",
    "daily_311['date'] = pd.to_datetime(daily_311['date'])\n",
    "checkin_daily['date'] = pd.to_datetime(checkin_daily['date'])\n",
    "taxi_daily['date'] = pd.to_datetime(taxi_daily['date'])\n",
    "weather_clean['DATE'] = pd.to_datetime(weather_clean['DATE'])\n",
    "\n",
    "# merge features and fill missing counts with 0\n",
    "X = X.merge(complaint_daily, on=[\"region_id\", \"date\"], how=\"left\")\n",
    "X = X.merge(sas_daily, on=[\"region_id\", \"date\"], how=\"left\")\n",
    "X = X.merge(daily_311, on=[\"region_id\", \"date\"], how=\"left\")\n",
    "X = X.merge(checkin_daily, on=[\"region_id\", \"date\"], how=\"left\")\n",
    "X = X.merge(taxi_daily, on=[\"region_id\", \"date\"], how=\"left\")\n",
    "X = X.merge(weather_clean, left_on=\"date\", right_on=\"DATE\", how=\"left\")\n",
    "\n",
    "for col in [\"complaint_count\", \"sas_count\", \"311_count\", \"checkin_count\", \"taxi_count\"]:\n",
    "    X[col] = X[col].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f60f377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('data/FEATURE_MATRIX.csv')\n",
    "grid_active.to_file(\"data/nyc_grid_2km_active.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299162b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE COMPLAINTS AND TAXI DATASETS\n",
    "\n",
    "# Load existing feature matrix (defines region set and date window)\n",
    "X = pd.read_csv(\"data/FEATURE_MATRIX.csv\", index_col=0, parse_dates=[\"date\"])\n",
    "\n",
    "active_regions = set(X[\"region_id\"].unique())\n",
    "date_min = X[\"date\"].min().normalize()\n",
    "date_max = X[\"date\"].max().normalize()\n",
    "\n",
    "# Build exact already existing index, so updates cannot change the shape\n",
    "full_idx = pd.MultiIndex.from_frame(X[[\"region_id\", \"date\"]])\n",
    "\n",
    "def align_to_X(daily_df, value_col):\n",
    "    # Make types consistent\n",
    "    daily_df = daily_df.copy()\n",
    "    daily_df[\"region_id\"] = daily_df[\"region_id\"].astype(X[\"region_id\"].dtype, errors=\"ignore\")\n",
    "    daily_df[\"date\"] = pd.to_datetime(daily_df[\"date\"]).dt.normalize()\n",
    "\n",
    "    # Match notebook semantics: only active regions +\n",
    "    #  within the X date range\n",
    "    daily_df = daily_df[\n",
    "        daily_df[\"region_id\"].isin(active_regions)\n",
    "        & (daily_df[\"date\"] >= date_min)\n",
    "        & (daily_df[\"date\"] <= date_max)\n",
    "    ]\n",
    "\n",
    "    # Reindex to exactly X's (region_id, date) pairs, fill missing with 0\n",
    "    s = (daily_df\n",
    "         .set_index([\"region_id\", \"date\"])[value_col]\n",
    "         .reindex(full_idx, fill_value=0))\n",
    "    out = s.reset_index()\n",
    "    out.columns = [\"region_id\", \"date\", value_col]\n",
    "    return out\n",
    "\n",
    "# complaint_daily must have columns: region_id, date, complaint_count\n",
    "# taxi_daily must have columns: region_id, date, taxi_count\n",
    "complaint_aligned = align_to_X(complaint_daily, \"complaint_count\")\n",
    "taxi_aligned = align_to_X(taxi_daily, \"taxi_count\")\n",
    "\n",
    "# Merge in the updated columns only\n",
    "X = X.drop(columns=[\"complaint_count\", \"taxi_count\"], errors=\"ignore\")\n",
    "X = X.merge(complaint_aligned, on=[\"region_id\", \"date\"], how=\"left\")\n",
    "X = X.merge(taxi_aligned, on=[\"region_id\", \"date\"], how=\"left\")\n",
    "\n",
    "X.to_csv(\"data/FEATURE_MATRIX.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4acd47b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
